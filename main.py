# main.py
import pandas as pd
import json
import os
import re

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from scraping.givemeoc_scraper import GiveMeOcScraper
from scraping.tianyancha_client import TianyanchaClient
from nlp.standardize import process_jobs_dataframe
from nlp.entity_extraction import add_entities_to_dataframe
from matching.similarity import rank_jobs_by_keyword
from scraping.wechat_rss_scraper import WechatRssScraper

def run_job_agent(user_keyword, salary_range=None, use_cache=True):
    """
    è¿è¡ŒAIæ±‚èŒä»£ç†çš„æ ¸å¿ƒæµç¨‹
    :param user_keyword: ç”¨æˆ·è¾“å…¥çš„èŒä½å…³é”®å­—
    :param salary_range: ä¸€ä¸ªåŒ…å«(min_salary, max_salary)çš„å…ƒç»„ï¼Œæˆ–None
    :param use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    """
    print("="*50)
    print(f"ðŸš€ AIæ±‚èŒä»£ç†å¯åŠ¨ï¼Œæ­£åœ¨ä¸ºå…³é”®å­— '{user_keyword}' æœç´¢å·¥ä½œ...")
    if salary_range:
        print(f"è–ªèµ„èŒƒå›´è¦æ±‚: {salary_range[0]} - {salary_range[1]} å…ƒ/æœˆ")
    
    # --- 1. æ•°æ®èŽ·å– ---
    print("\n[STEP 1/5] å¼€å§‹èŽ·å–èŒä½æ•°æ®...")
    raw_jobs = []
    os.makedirs(os.path.dirname(JOBS_DATA_PATH), exist_ok=True)
    # ç¼“å­˜æ–‡ä»¶åçŽ°åœ¨åªä¸Žå…³é”®å­—ç›¸å…³
    cache_file_path = f"{JOBS_DATA_PATH.split('.')[0]}_{user_keyword.replace(' ', '_')}.json"
    
    if use_cache:
        try:
            with open(cache_file_path, 'r', encoding='utf-8') as f:
                raw_jobs = json.load(f)
            print(f"å·²ä»Žç¼“å­˜ '{cache_file_path}' åŠ è½½ {len(raw_jobs)} æ¡æ•°æ®ã€‚")
        except FileNotFoundError:
            print(f"æœªæ‰¾åˆ°é’ˆå¯¹ '{user_keyword}' çš„ç¼“å­˜æ–‡ä»¶ã€‚")
            use_cache = False
            
    if not use_cache or not raw_jobs:
        print("æ‰§è¡Œå®žæ—¶æ•°æ®èŽ·å–...")
        scrapers = [GiveMeOcScraper(), TianyanchaClient()] 
        all_fetched_jobs = []
        for scraper in scrapers:
            fetched = scraper.scrape(keyword=user_keyword)
            all_fetched_jobs.extend(fetched)
        raw_jobs = all_fetched_jobs
        
        with open(cache_file_path, 'w', encoding='utf-8') as f:
            json.dump(raw_jobs, f, ensure_ascii=False, indent=4)
        print(f"æ•°æ®å·²èŽ·å–å¹¶ç¼“å­˜åˆ° '{cache_file_path}'ã€‚")

    if not raw_jobs:
        print("æ‰€æœ‰æ•°æ®æºå‡æœªèƒ½èŽ·å–ä»»ä½•èŒä½ä¿¡æ¯ã€‚ç¨‹åºé€€å‡ºã€‚")
        return

    # --- 2. æ•°æ®å¤„ç† ---
    print("\n[STEP 2/5] æ­£åœ¨æ¸…æ´—å’Œæ ‡å‡†åŒ–æ•°æ®...")
    df = process_jobs_dataframe(raw_jobs)
    
    # --- 3. æ–°å¢žæ­¥éª¤ï¼šè–ªèµ„ç­›é€‰ ---
    if salary_range and 'estimated_salary' in df.columns:
        print("\n[STEP 3/5] æ­£åœ¨æŒ‰è–ªèµ„èŒƒå›´è¿›è¡Œç­›é€‰...")
        min_sal, max_sal = salary_range
        original_count = len(df)
        # Drop rows where salary couldn't be estimated for a clean filter
        df_with_salary = df.dropna(subset=['estimated_salary']).copy()
        # Apply the filter
        df = df_with_salary[
            (df_with_salary['estimated_salary'] >= min_sal) & 
            (df_with_salary['estimated_salary'] <= max_sal)
        ]
        print(f"ç­›é€‰å®Œæˆï¼Œ{original_count}ä¸ªèŒä½ä¸­æœ‰{len(df)}ä¸ªç¬¦åˆè–ªèµ„è¦æ±‚ã€‚")

    if df.empty:
        print("ç»è¿‡ç­›é€‰åŽï¼Œæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„èŒä½ã€‚ç¨‹åºé€€å‡ºã€‚")
        return

    # --- 4. NLPåˆ†æž ---
    print("\n[STEP 4/5] æ­£åœ¨è¿›è¡ŒNLPåˆ†æž...")
    df = add_entities_to_dataframe(df)
    
    # --- 5. åŒ¹é…ä¸ŽæŽ’åº ---
    print("\n[STEP 5/5] æ­£åœ¨æ ¹æ®æ‚¨çš„å…³é”®å­—è¿›è¡ŒåŒ¹é…å’ŒæŽ’åº...")
    ranked_df = rank_jobs_by_keyword(df, user_keyword)

    # --- ç»“æžœå±•ç¤º ---
    print("\n" + "="*50)
    print("ðŸŽ‰ åŒ¹é…å®Œæˆï¼ä»¥ä¸‹æ˜¯æœ€ç›¸å…³çš„5ä¸ªèŒä½ï¼š")
    print("="*50)
    
    pd.set_option('display.max_colwidth', None)
    pd.set_option('display.width', 200)
    
    if ranked_df.empty:
        print("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„èŒä½ã€‚")
    else:
        display_columns = ['title', 'company', 'source', 'estimated_salary', 'similarity_score', 'url']
        display_columns = [col for col in display_columns if col in ranked_df.columns]
        
        for index, row in ranked_df.head(5).iterrows():
            print(f"ã€èŒä½{index + 1}ã€‘: {row.get('title', 'N/A')}")
            print(f"  å…¬å¸: {row.get('company', 'N/A')}")
            print(f"  æ¥æº: {row.get('source', 'N/A')}")
            # æ ¼å¼åŒ–è–ªèµ„è¾“å‡º
            salary = row.get('estimated_salary')
            salary_str = f"{salary:,.0f} å…ƒ/æœˆ" if pd.notna(salary) else "æœªæä¾›"
            print(f"  ä¼°ç®—æœˆè–ª: {salary_str}")
            print(f"  åŒ¹é…å¾—åˆ†: {row.get('similarity_score', 0.0):.4f}")
            print(f"  é“¾æŽ¥: {row.get('url', 'N/A')}")
            print("-" * 40)


if __name__ == "__main__":
    try:
        search_type = input("è¯·é€‰æ‹©æœç´¢ç±»åž‹: 1.æŒ‰èŒä½å…³é”®å­—æœç´¢  2.æŒ‰å…¬ä¼—å·åç§°æŠ“å–\nè¯·è¾“å…¥æ•°å­—: ")

        if search_type == '1':
            # ... æ­¤å¤„æ˜¯ä¹‹å‰æŒ‰å…³é”®å­—å’Œè–ªèµ„æœç´¢çš„é€»è¾‘ ...
            keywords = input("è¯·è¾“å…¥èŒä½å…³é”®å­— (å¯è¾“å…¥å¤šä¸ªï¼Œç”¨ç©ºæ ¼éš”å¼€): ")
            # ...
            if keywords:
                run_job_agent(user_keyword=keywords, salary_range=salary_range, use_cache=False)

        elif search_type == '2':
            # æ³¨æ„ï¼šRSSæŠ“å–éœ€è¦å…¬ä¼—å·çš„RSSé“¾æŽ¥ï¼Œè€Œä¸æ˜¯åç§°ã€‚
            # ç”¨æˆ·éœ€è¦è‡ªè¡Œæ‰¾åˆ°ç›®æ ‡å…¬ä¼—å·çš„RSSé“¾æŽ¥ã€‚
            # ä¾‹å¦‚ï¼Œä¸€äº›ç¬¬ä¸‰æ–¹æœåŠ¡å¯ä»¥ç”ŸæˆRSSé“¾æŽ¥ï¼Œå¦‚ https://rss.imzhao.com/
            rss_url = input("è¯·è¾“å…¥è¦æŠ“å–çš„å…¬ä¼—å·çš„RSSé“¾æŽ¥ (ä¾‹å¦‚: https://rss.imzhao.com/xxxxxxxx.xml): ")
            if rss_url:
                # ç›´æŽ¥è°ƒç”¨æ–°çš„RSSçˆ¬è™«
                rss_scraper = WechatRssScraper(rss_url=rss_url)
                raw_jobs = rss_scraper.scrape()

                if raw_jobs:
                    # èŽ·å–æ•°æ®åŽï¼Œå¯ä»¥èµ°åŒæ ·çš„æ•°æ®å¤„ç†å’Œå±•ç¤ºæµç¨‹
                    df = process_jobs_dataframe(raw_jobs)
                    df = add_entities_to_dataframe(df)
                    print("\n" + "="*50)
                    print(f"æ¥è‡ªRSSæºçš„æœ€æ–°æ‹›è˜ä¿¡æ¯:")
                    print("="*50)
                    print(df[['title', 'company', 'url']].head(10)) # ç®€å•å±•ç¤ºç»“æžœ
                else:
                    print("æœªèƒ½ä»Žè¯¥RSSæºèŽ·å–åˆ°æ‹›è˜ä¿¡æ¯ã€‚")
        else:
            print("æ— æ•ˆçš„è¾“å…¥ã€‚")

    except KeyboardInterrupt:
        print("\nç¨‹åºå·²ç”±ç”¨æˆ·ä¸­æ–­ã€‚")
    except Exception as e:
        print(f"\nç¨‹åºè¿è¡Œå‡ºé”™: {e}")
